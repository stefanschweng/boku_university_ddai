{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f3047d-b0f3-4179-8394-2a6eed58e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, VBox, HBox, Layout, Output, Label, Dropdown\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def plot_description(text):\n",
    "    print(f\"\\nDescription:\\n{text}\\n\")\n",
    "\n",
    "\n",
    "LOSS_A_LABEL = \"(A) x^2 + y^2\"\n",
    "LOSS_B_LABEL = \"(B) 0.1 * x^2 + y^2\"\n",
    "LOSS_C_LABEL = \"(C) 10 * x^2 + y^2\"\n",
    "\n",
    "\n",
    "# Global output widget for the 3D plot\n",
    "output_2d = Output(layout=Layout(width='50%', height='auto'))\n",
    "output_3d = Output(layout=Layout(width='50%', height='auto'))\n",
    "\n",
    "\n",
    "def quadratic_loss(x, y, conditioning=1.0):\n",
    "    \"\"\"A quadratic loss function with adjustable conditioning.\"\"\"\n",
    "    return 0.5 * (conditioning * x**2 + y**2)\n",
    "\n",
    "\n",
    "def gradient_descent(start_x, start_y, step_size, epochs, conditioning):\n",
    "    \"\"\"Simulate gradient descent on the quadratic loss function.\"\"\"\n",
    "    x, y = start_x, start_y\n",
    "    path = [(x, y)]\n",
    "    for _ in range(epochs):\n",
    "        grad_x = conditioning * x\n",
    "        grad_y = y\n",
    "        x -= step_size * grad_x\n",
    "        y -= step_size * grad_y\n",
    "        path.append((x, y))\n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def plot_2d_convergence(step_size, epochs, loss_function=LOSS_A_LABEL):\n",
    "    \"\"\"Plot the 2D contour and gradient descent path.\"\"\"\n",
    "    \n",
    "    if loss_function == LOSS_A_LABEL:\n",
    "        conditioning = 1.0\n",
    "    elif loss_function == LOSS_B_LABEL:\n",
    "        conditioning = 0.1\n",
    "    else:\n",
    "        conditioning = 10.0\n",
    "\n",
    "    with output_2d:\n",
    "        clear_output(wait=True)\n",
    "        x = np.linspace(-2, 2, 100)\n",
    "        y = np.linspace(-2, 2, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = quadratic_loss(X, Y, conditioning)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 6))\n",
    "        contour = ax.contour(X, Y, Z, levels=20, cmap=\"viridis\")\n",
    "        plt.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "        # Gradient descent path\n",
    "        path = gradient_descent(1.8, 1.5, step_size, epochs, conditioning)\n",
    "        ax.plot(0, 0, 'bx', markersize=10, markeredgewidth=3, label=\"Minimum (x: 0, y: 0)\")\n",
    "        ax.plot(path[:, 0], path[:, 1], \"ro-\", markersize=4, linewidth=1, label=\"Optimization Path\")\n",
    "        ax.plot(path[0, 0], path[0, 1], \"*\", markersize=10, color=\"black\", label=\"Start\")\n",
    "\n",
    "        ax.set_title(f\"2D Loss Contour and Optimization Path (\\\"Top View\\\")\")\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_xlim([-2, 2])\n",
    "        ax.set_ylim([-2, 2])\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_3d_loss(loss_function=LOSS_A_LABEL):\n",
    "    \"\"\"Plot the 3D surface of the quadratic loss function.\"\"\"\n",
    "\n",
    "    if loss_function == LOSS_A_LABEL:\n",
    "        conditioning = 1.0\n",
    "    elif loss_function == LOSS_B_LABEL:\n",
    "        conditioning = 0.1\n",
    "    else:\n",
    "        conditioning = 10.0\n",
    "\n",
    "    with output_3d:\n",
    "        clear_output(wait=True)\n",
    "        x = np.linspace(-2, 2, 100)\n",
    "        y = np.linspace(-2, 2, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = quadratic_loss(X, Y, conditioning)\n",
    "\n",
    "        fig = plt.figure(figsize=(7, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.8)\n",
    "        ax.plot(0, 0, \"ro\", markersize=12, markeredgewidth=3, label=\"Minimum (x: 0, y: 0)\")\n",
    "\n",
    "        ax.set_title(f\"3D Loss Function [{loss_function}]\")\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_zlabel(\"Loss\")\n",
    "        ax.set_box_aspect(None, zoom=0.87)  # avoid cutting off z-axis label\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def update_2d_plot(change):\n",
    "    if change['name'] == 'value':\n",
    "        plot_2d_convergence(step_slider.value, epochs_slider.value, loss_function_dropdown.value)\n",
    "\n",
    "\n",
    "def update_3d_plot(change):\n",
    "    \"\"\"Update the 3D plot only when conditioning changes.\"\"\" \n",
    "    if change['name'] == 'value':\n",
    "        plot_3d_loss(loss_function_dropdown.value)\n",
    "\n",
    "\n",
    "label = Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 8px 0\"))\n",
    "\n",
    "# Sliders\n",
    "step_slider = FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description=\"Learning Rate\", layout=Layout(width=\"300px\"))\n",
    "epochs_slider = IntSlider(value=20, min=1, max=100, step=1, description=\"Epochs\", layout=Layout(width=\"300px\"))\n",
    "loss_function_dropdown = Dropdown(options=[LOSS_A_LABEL, LOSS_B_LABEL, LOSS_C_LABEL], value=LOSS_A_LABEL, description=\"Loss func.\", layout=Layout(width=\"300px\"))\n",
    "\n",
    "# Add left margin (indent) to the widgets you want to indent\n",
    "margin_left = 40\n",
    "step_slider.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "epochs_slider.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "loss_function_dropdown.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "\n",
    "# Observe changes in conditioning\n",
    "step_slider.observe(update_2d_plot)\n",
    "epochs_slider.observe(update_2d_plot)\n",
    "loss_function_dropdown.observe(update_3d_plot, names=\"value\")\n",
    "loss_function_dropdown.observe(update_2d_plot, names=\"value\")\n",
    "\n",
    "plot_3d_loss(LOSS_A_LABEL)\n",
    "\n",
    "# Display\n",
    "ui = VBox([label, step_slider, epochs_slider, loss_function_dropdown,\n",
    "           HBox([output_2d, output_3d])])\n",
    "\n",
    "\n",
    "def gradient_descent_3D_convergence_interact():\n",
    "    print(\n",
    "        \"\\nDescription:\\n\"\n",
    "        \"Gradient descent example in 3D. Convergence and Loss Function Visualization.\\n\"\n",
    "        \"Use the sliders to explore how learning rate, number of epochs, and conditioning of the loss function affect the optimization path.\\n\\n\"\n",
    "        \"- Learning Rate: Controls the magnitude of each update. Too large may cause divergence; too small may slow convergence.\\n\"\n",
    "        \"- Epochs: Determines the number of optimization steps taken.\\n\"\n",
    "        \"- Loss function: Adjusts the shape of the loss function. Perfect conditioning (A) results in a circular contour and a straight optimization path, while poor conditioning (B or C) creates an elliptical contour, leading to zig-zag convergence.\\n\\n\"\n",
    "        \"- Left Plot (2D): Shows the contour of the loss function and the optimization path of gradient descent.\\n\"\n",
    "        \"- Right Plot (3D): Displays the 3D surface of the loss function, highlighting how conditioning changes its shape.\\n\\n\"\n",
    "        \"Notice how the optimization path and loss landscape interact and how it takes more epochs to converge to the function minimum (x: 0, y:0) for ill-conditioned loss functions.\\n\"\n",
    "    )\n",
    "    display(ui)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
