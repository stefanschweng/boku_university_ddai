{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb56c9bd-d94d-495e-8dcf-b25f54eff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipywidgets import interactive, FloatSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def plot_description(text):\n",
    "    print(f\"\\nDescription:\\n{text}\\n\")\n",
    "\n",
    "\n",
    "def binary_classification_demo(separation=1.0):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create binary dataset\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    X, y = make_classification(\n",
    "        n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
    "        n_clusters_per_class=1, class_sep=separation, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train logistic regression model\n",
    "    # ---------------------------------------------------\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Visualize decision boundary\n",
    "    # ---------------------------------------------------\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, 1-Z, cmap=\"RdYlBu\", alpha=0.5)\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], color=\"blue\", label=\"Class 0\", edgecolor=\"black\", linewidth=0.5)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], color=\"red\", label=\"Class 1\", edgecolor=\"black\", linewidth=0.5)\n",
    "    plt.title(\"Binary Classification with Logistic Regression\", fontsize=16)\n",
    "    plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "    plt.ylabel(\"Feature 2\", fontsize=14)\n",
    "    plt.legend(title=\"Ground Truth\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def binary_classification_demo_interact():\n",
    "    # ---------------------------------------------------\n",
    "    # 4. Visualize Binary Cross-Entropy loss function\n",
    "    # ---------------------------------------------------\n",
    "    p = np.linspace(0.001, 0.999, 200)\n",
    "    loss_positive = -np.log(p)\n",
    "    loss_negative = -np.log(1 - p)\n",
    "\n",
    "    plot_description(\"We are looking at a binary classification problem (i.e., assigning a sample to one of two classes). The Figure\"\n",
    "                     \" below shows an example visualization of the Binary Cross-Entropy (BCE) loss function that was used to train the \"\n",
    "                     \"logistic regression model. The BCE function calculates the loss values based on the orange line (for samples of class 0) \"\n",
    "                    \"or based on the blue line (for samples of class 1).\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(p, loss_positive, label=\"Loss (True Label = 1)\", linewidth=2)\n",
    "    plt.plot(p, loss_negative, label=\"Loss (True Label = 0)\", linewidth=2)\n",
    "    plt.title(\"Binary Cross-Entropy Loss\", fontsize=14)\n",
    "    plt.xlabel(\"Predicted Probability (p)\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plot_description(\"Looking at the plot below, the blue and rot dots represent training samples of class 0 and class 1 respectively.\"\n",
    "                     \" The background shows prediction probabilities of the logisitic regression model.\"\n",
    "                     \" Bluish background: High probability for Class 0. Reddish background: High probability for Class 1. \\n\"\n",
    "                     \"The model assigns class 0 or 1 to a new \\\"unseen\\\" sample, based on these probabilities. \"\n",
    "                     \"Notice how prediction uncertainty increases as training samples of both classes get mixed (play around with the slider below).\")\n",
    "    \n",
    "    # --- Interactive control ---\n",
    "    sep_slider = FloatSlider(\n",
    "        value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "        description=\"Move Samples\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "    interactive_plot = interactive(binary_classification_demo, separation=sep_slider)\n",
    "\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6f9d1-754a-45fa-9a85-e9c6f1949baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
