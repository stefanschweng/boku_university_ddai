{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f780a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interactive ML playground for the Forest Covertype dataset (scikit-learn \"covtype\")\n",
    "# Single-cell Jupyter code using ipywidgets\n",
    "# Requirements: scikit-learn, ipywidgets, pandas, numpy\n",
    "# If widgets do not render, ensure: pip install ipywidgets && jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    ")\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "#########################################################################################################################\n",
    "####################################### DATASET #########################################################################\n",
    "#########################################################################################################################\n",
    "# -----------------------------\n",
    "# Load dataset once\n",
    "# -----------------------------\n",
    "covtype = fetch_covtype(as_frame=False)\n",
    "X_all, y_all = covtype.data, covtype.target\n",
    "X_all = X_all.astype(np.float32)\n",
    "\n",
    "# Stratified split to preserve class distribution\n",
    "X, _, y, _ = train_test_split(\n",
    "    X_all, y_all, train_size=0.06, stratify=y_all, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets\n",
    "# -----------------------------\n",
    "class_groups = {\n",
    "    \"Subset A\": [1, 2, 5],\n",
    "    \"Subset B\": [4, 5, 7],\n",
    "    \"Subset C\": [3, 6, 7],\n",
    "}\n",
    "\n",
    "group_dropdown = widgets.Dropdown(\n",
    "    options=list(class_groups.keys()),\n",
    "    value=\"Subset A\",\n",
    "    description=\"Subset:\",\n",
    ")\n",
    "\n",
    "viz_points = 150 #widgets.IntSlider(\n",
    "#    value=600, min=200, max=3000, step=100, description=\"Max points (pairplot)\"\n",
    "#)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Forest Covertype Subset Loading and Visualization</h2>\"),\n",
    "    widgets.HBox([group_dropdown]),\n",
    "    #widgets.HBox([viz_points, show_pairplot]),\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: balanced subsample per class\n",
    "# -----------------------------\n",
    "def balanced_subsample(Xs, ys, max_total=600, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    classes, counts = np.unique(ys, return_counts=True)\n",
    "    per_class = max_total // len(classes) if len(classes) > 0 else max_total\n",
    "    idx_all = []\n",
    "    for c in classes:\n",
    "        idx_c = np.where(ys == c)[0]\n",
    "        take = min(len(idx_c), per_class)\n",
    "        if take > 0:\n",
    "            chosen = rng.choice(idx_c, size=take, replace=False)\n",
    "            idx_all.append(chosen)\n",
    "    if len(idx_all) == 0:\n",
    "        return Xs[:0], ys[:0]\n",
    "    idx_all = np.concatenate(idx_all)\n",
    "    rng.shuffle(idx_all)\n",
    "    return Xs[idx_all], ys[idx_all]\n",
    "\n",
    "# -----------------------------\n",
    "# Main runner\n",
    "# -----------------------------\n",
    "def run_dataset(*args):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        selected = class_groups[group_dropdown.value]\n",
    "        mask = np.isin(y, selected)\n",
    "        X_sub = X[mask]\n",
    "        y_sub = y[mask]\n",
    "\n",
    "        global X_train, X_test, y_train, y_test\n",
    "        if group_dropdown.value == \"Subset C\":\n",
    "            X_sub, _, y_sub, _ = train_test_split(\n",
    "                X_sub, y_sub, train_size=0.5, stratify=y_sub, random_state=42\n",
    "            )\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_sub, y_sub, test_size=0.5, stratify=y_sub, random_state=42\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_sub, y_sub, test_size=0.2, stratify=y_sub, random_state=42\n",
    "            )\n",
    "\n",
    "        present_classes = np.unique(y_sub)\n",
    "        print(\"\\n=== Dataset Subset Information ===\")\n",
    "        print(f\"Selected subset: {group_dropdown.value}\")\n",
    "        print(f\"Requested classes: {selected}\")\n",
    "        print(f\"Present classes in dataset: {present_classes.tolist()}\")\n",
    "        if len(X_sub) == 0:\n",
    "            print(\"No samples found for the selected class group. Try a different selection.\")\n",
    "            return\n",
    "\n",
    "        # Summary\n",
    "        counts = {int(c): int((y_sub == c).sum()) for c in present_classes}\n",
    "        print(f\"Subset size: {len(X_sub)} samples\")\n",
    "        print(f\"Class counts: {counts}\")\n",
    "\n",
    "        # Correlation heatmap for first 10 features (across the subset)\n",
    "        #cols = [f\"f{i}\" for i in range(10)]\n",
    "        #cols = [\n",
    "        #    'Elevation',\n",
    "        #    'Aspect',\n",
    "        #    'Slope',\n",
    "        #    'HDist_Hydro',\n",
    "        #    'VDist_Hydro',\n",
    "        #    'HDist_Road',\n",
    "        #    'Hillsh_9am',\n",
    "        #    'Hillsh_Noon',\n",
    "        #    'Hillsh_3pm',\n",
    "        #    'HDist_FirePts'\n",
    "        #]\n",
    "        #df10 = pd.DataFrame(X_sub[:, :10], columns=cols)\n",
    "        #corr = df10.corr(method=\"pearson\")\n",
    "\n",
    "        #X_vis, y_vis = balanced_subsample(X_sub, y_sub, max_total=viz_points, seed=42)\n",
    "        #if len(X_vis) == 0:\n",
    "        #    print(\"Not enough samples to create pairplot.\")\n",
    "        #    return\n",
    "        #df_vis = pd.DataFrame(X_vis[:, :10], columns=cols)\n",
    "        #df_vis[\"class\"] = y_vis.astype(int)\n",
    "\n",
    "        # Use corner=True to reduce complexity of the grid\n",
    "        #g = sns.pairplot(\n",
    "        #    df_vis, vars=cols, hue=\"class\", corner=True,\n",
    "        #    plot_kws={\"s\": 10, \"alpha\": 0.6},\n",
    "        #    diag_kind=\"kde\"\n",
    "        #)\n",
    "        #g.fig.set_size_inches(12, 12)\n",
    "        #g.fig.suptitle(\"Pairplot (first 10 features, colored by class, subsampled)\", y=1.02)\n",
    "        #plt.show()\n",
    "\n",
    "group_dropdown.observe(run_dataset, names='value')\n",
    "\n",
    "\n",
    "data_ui = widgets.VBox([controls, output])\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "####################################### MODEL ###########################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: build preprocessor\n",
    "# -----------------------------\n",
    "def make_preprocessor(choice):\n",
    "    if choice == \"None\":\n",
    "        return None\n",
    "    if choice == \"Center\":\n",
    "        return StandardScaler(with_mean=True, with_std=False)\n",
    "    if choice == \"Scale\":\n",
    "        return StandardScaler(with_mean=False, with_std=True)\n",
    "    if choice == \"Center & Scale\":\n",
    "        return StandardScaler()\n",
    "    if choice == \"Normalize (L2)\":\n",
    "        return Normalizer(norm=\"l2\")\n",
    "    raise ValueError(f\"Unknown preprocessing option: {choice}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: balance training set by random undersampling to smallest class count\n",
    "# -----------------------------\n",
    "def balance_training(Xt, yt, minmax, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    classes, counts = np.unique(yt, return_counts=True)\n",
    "    if minmax == \"min\":\n",
    "        n = counts.min()\n",
    "    elif minmax == \"max\":\n",
    "        n = counts.max()\n",
    "    #n = int(counts.mean())\n",
    "    idx_list = []\n",
    "    for c in classes:\n",
    "        idx_c = np.where(yt == c)[0]\n",
    "        if len(idx_c) < n:\n",
    "            chosen = rng.choice(idx_c, size=n, replace=True)\n",
    "        else:\n",
    "            chosen = rng.choice(idx_c, size=n, replace=False)\n",
    "        idx_list.append(chosen)\n",
    "    idx_all = np.concatenate(idx_list)\n",
    "    rng.shuffle(idx_all)\n",
    "    return Xt[idx_all], yt[idx_all]\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets: Data Preparation\n",
    "# -----------------------------\n",
    "preproc_dropdown = widgets.Dropdown(\n",
    "    options=[\"None\", \"Center\", \"Scale\", \"Center & Scale\", \"Normalize (L2)\"],\n",
    "    value=\"Center & Scale\",\n",
    "    description=\"Preprocess:\",\n",
    ")\n",
    "\n",
    "#balance_checkbox = widgets.Checkbox(\n",
    "#    value=False, description=\"Balance training samples\"\n",
    "#)\n",
    "balance_dropdown = widgets.Dropdown(\n",
    "    options=[\"None\", \"Oversample minority class\", \"Undersample majority class\"],\n",
    "    value=\"None\",\n",
    "    description=\"Balancing:\"\n",
    ")\n",
    "\n",
    "\n",
    "data_prep_box = widgets.VBox([preproc_dropdown, balance_dropdown])\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets: Feature Selection (columns 0..9)\n",
    "# -----------------------------\n",
    "#feature_checkboxes = [widgets.Checkbox(value=True, description=f\"Col {i}\") for i in range(10)]\n",
    "feature_checkboxes = [widgets.Checkbox(value=True, description=f\"{i}\") for i in covtype.feature_names[:10]]\n",
    "feature_wild = widgets.Checkbox(value=True, description=\"Wilderness Area\")\n",
    "feature_soil = widgets.Checkbox(value=True, description=\"Soil Type\")\n",
    "# Arrange in two rows for readability\n",
    "feature_selection_box = widgets.VBox([\n",
    "    widgets.HBox(feature_checkboxes[:3]),\n",
    "    widgets.HBox(feature_checkboxes[3:6]),\n",
    "    widgets.HBox(feature_checkboxes[6:9]),\n",
    "    widgets.HBox(feature_checkboxes[9:] + [feature_wild, feature_soil]),\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets: Model Selection\n",
    "# -----------------------------\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\"Logistic Regression\", \"MLP\", \"Decision Tree\", \"Random Forest\"],\n",
    "    value=\"Logistic Regression\",\n",
    "    description=\"Model:\",\n",
    ")\n",
    "\n",
    "slider_style_props = {'style': {'description_width': '140px'}, 'layout': widgets.Layout(width='360px')}\n",
    "\n",
    "# Model-dependent complexity controls\n",
    "logreg_degree = widgets.IntSlider(value=1, min=1, max=6, step=1, description=\"Polynomial Degree\", **slider_style_props)\n",
    "mlp_neurons = widgets.IntSlider(value=64, min=8, max=256, step=8, description=\"# Neurons\", **slider_style_props)\n",
    "dt_max_depth = widgets.IntSlider(value=20, min=2, max=100, step=2, description=\"Max depth\", **slider_style_props)\n",
    "rf_n_estimators = widgets.IntSlider(value=100, min=10, max=300, step=10, description=\"# Trees\", **slider_style_props)\n",
    "rf_max_depth = widgets.IntSlider(value=20, min=2, max=50, step=1, description=\"Max depth\", **slider_style_props)\n",
    "complexity_box = widgets.VBox([])\n",
    "\n",
    "def update_complexity_controls(*args):\n",
    "    mdl = model_dropdown.value\n",
    "    if mdl == \"Logistic Regression\":\n",
    "        #complexity_box.children = [widgets.HTML(\"<i>No complexity slider for Logistic Regression.</i>\")]\n",
    "        complexity_box.children = [logreg_degree]\n",
    "    elif mdl == \"MLP\":\n",
    "        complexity_box.children = [mlp_neurons]\n",
    "    elif mdl == \"Decision Tree\":\n",
    "        complexity_box.children = [dt_max_depth]\n",
    "    elif mdl == \"Random Forest\":\n",
    "        complexity_box.children = [rf_n_estimators, rf_max_depth]\n",
    "\n",
    "model_dropdown.observe(update_complexity_controls, names=\"value\")\n",
    "update_complexity_controls()\n",
    "\n",
    "model_selection_box = widgets.VBox([model_dropdown, complexity_box])\n",
    "\n",
    "# -----------------------------\n",
    "# Widgets: Model Training & Hyperparameter Tuning (regularization)\n",
    "# -----------------------------\n",
    "# Logistic Regression regularization\n",
    "#lr_C = widgets.FloatLogSlider(\n",
    "#    value=1.0, base=10, min=-2, max=2, step=0.1, description=\"LR C (1/λ)\"\n",
    "#)\n",
    "lr_penalty = widgets.Dropdown(\n",
    "    #options=[\"None\", 'l2', 'l1', 'elasticnet'],\n",
    "    options=[\"None\", 'l2'],\n",
    "    value=\"None\",\n",
    "    description=\"Penalty:\",\n",
    "    **slider_style_props\n",
    ")\n",
    "\n",
    "# MLP regularization\n",
    "mlp_alpha = widgets.FloatLogSlider(\n",
    "    value=1e-2, base=10, min=-8, max=0, step=0.1, description=\"MLP α (L2)\", **slider_style_props\n",
    ")\n",
    "\n",
    "# Decision Tree regularization (cost-complexity pruning alpha)\n",
    "dt_ccp_alpha = widgets.FloatSlider(\n",
    "    value=0.0, min=0.0, max=0.02, step=0.001, description=\"Node Pruning (α)\", readout_format='.3f', **slider_style_props\n",
    ")\n",
    "\n",
    "# Random Forest regularization-ish\n",
    "rf_min_samples_leaf = widgets.IntSlider(\n",
    "    value=1, min=1, max=20, step=1, description=\"min_samples_leaf\", **slider_style_props\n",
    ")\n",
    "\n",
    "tuning_box = widgets.VBox([])\n",
    "\n",
    "def update_tuning_controls(*args):\n",
    "    mdl = model_dropdown.value\n",
    "    if mdl == \"Logistic Regression\":\n",
    "        tuning_box.children = [lr_penalty]\n",
    "    elif mdl == \"MLP\":\n",
    "        tuning_box.children = [mlp_alpha]\n",
    "    elif mdl == \"Decision Tree\":\n",
    "        tuning_box.children = [dt_ccp_alpha]\n",
    "    elif mdl == \"Random Forest\":\n",
    "        tuning_box.children = [rf_min_samples_leaf]\n",
    "\n",
    "model_dropdown.observe(update_tuning_controls, names=\"value\")\n",
    "update_tuning_controls()\n",
    "\n",
    "# -----------------------------\n",
    "# Containers with block titles\n",
    "# -----------------------------\n",
    "data_prep_section = widgets.VBox([widgets.HTML(\"<h3>Data Preparation</h3>\"), data_prep_box])\n",
    "feature_selection_section = widgets.VBox([widgets.HTML(\"<h3>Feature Selection</h3>\"), feature_selection_box])\n",
    "model_full_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>Model</h4>\"),\n",
    "    model_dropdown,\n",
    "    widgets.HTML(\"<h4>Tuning / Regularization</h4>\"),\n",
    "    complexity_box,\n",
    "    tuning_box\n",
    "])\n",
    "\n",
    "# Optionally use Accordion\n",
    "accordion = widgets.Accordion(children=[\n",
    "    data_prep_box, feature_selection_box, model_full_box\n",
    "])\n",
    "accordion.set_title(0, \"Data Preparation\")\n",
    "accordion.set_title(1, \"Feature Selection\")\n",
    "accordion.set_title(2, \"Model Selection, Parameters & Regularization\")\n",
    "#accordion = widgets.VBox([\n",
    "#    data_prep_box, feature_selection_box, model_selection_box, tuning_box\n",
    "#])\n",
    "\n",
    "# -----------------------------\n",
    "# Output and run controls\n",
    "# -----------------------------\n",
    "run_button = widgets.Button(description=\"Train\", button_style=\"success\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# -----------------------------\n",
    "# Build model pipeline based on UI\n",
    "# -----------------------------\n",
    "def build_classifier():\n",
    "    mdl = model_dropdown.value\n",
    "    if mdl == \"Logistic Regression\":\n",
    "        clf = LogisticRegression(\n",
    "            #C=lr_C.value,\n",
    "            penalty=lr_penalty.value if lr_penalty.value != 'None' else None,\n",
    "            #l1_ratio=0.5 if lr_penalty.value == 'elastic_net' else 0.0,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=100,\n",
    "            #multi_class=\"auto\",\n",
    "            random_state=42\n",
    "        )\n",
    "    elif mdl == \"MLP\":\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=(mlp_neurons.value,mlp_neurons.value, mlp_neurons.value),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=mlp_alpha.value,\n",
    "            max_iter=100,\n",
    "            early_stopping=False,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif mdl == \"Decision Tree\":\n",
    "        clf = DecisionTreeClassifier(\n",
    "            max_depth=dt_max_depth.value,\n",
    "            ccp_alpha=dt_ccp_alpha.value,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif mdl == \"Random Forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=rf_n_estimators.value,\n",
    "            max_depth=rf_max_depth.value,\n",
    "            min_samples_leaf=rf_min_samples_leaf.value,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    return clf\n",
    "\n",
    "# -----------------------------\n",
    "# Run experiment on button click\n",
    "# -----------------------------\n",
    "def run_experiment(*args):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Feature selection: collect selected columns\n",
    "        selected_cols = [i for i, cb in enumerate(feature_checkboxes) if cb.value]\n",
    "        if feature_wild.value:\n",
    "            selected_cols +=  list(range(10,14))\n",
    "        if feature_soil.value:\n",
    "            selected_cols +=  list(range(14,54))\n",
    "            \n",
    "        if len(selected_cols) == 0:\n",
    "            print(\"Please select at least one feature.\")\n",
    "            return\n",
    "\n",
    "        # Slice features\n",
    "        Xtr = X_train[:, selected_cols]\n",
    "        Xte = X_test[:, selected_cols]\n",
    "\n",
    "        # Balance training set if requested\n",
    "        if balance_dropdown.value == \"Oversample minority class\":\n",
    "            Xtr_bal, ytr_bal = balance_training(Xtr, y_train, minmax=\"max\", seed=42)\n",
    "        elif balance_dropdown.value == \"Undersample majority class\":\n",
    "            Xtr_bal, ytr_bal = balance_training(Xtr, y_train, minmax=\"min\", seed=42)\n",
    "        else:\n",
    "            Xtr_bal, ytr_bal = Xtr, y_train\n",
    "\n",
    "        # Build pipeline\n",
    "        preproc = make_preprocessor(preproc_dropdown.value)\n",
    "        clf = build_classifier()\n",
    "        steps = []\n",
    "        if model_dropdown.value == \"Logistic Regression\":\n",
    "            steps.append((\"poly\", PolynomialFeatures(degree=logreg_degree.value, include_bias=False)))\n",
    "        if preproc is not None:\n",
    "            steps.append((\"preprocess\", preproc))\n",
    "        steps.append((\"clf\", clf))\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        # Train\n",
    "        t0 = time.perf_counter()\n",
    "        pipe.fit(Xtr_bal, ytr_bal)\n",
    "        t_train = time.perf_counter() - t0\n",
    "\n",
    "        # Predict\n",
    "        ytr_pred = pipe.predict(Xtr_bal)\n",
    "        t1 = time.perf_counter()\n",
    "        y_pred = pipe.predict(Xte)\n",
    "        t_pred = time.perf_counter() - t1\n",
    "\n",
    "        # Metrics\n",
    "        tr_acc = accuracy_score(ytr_bal, ytr_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1m = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Report\n",
    "        print(\"\\n=== Configuration ===\")\n",
    "        active_feat_names = [cb.description for cb in feature_checkboxes if cb.value]\n",
    "        if feature_wild.value:\n",
    "            active_feat_names.append(\"Wilderness Area\")\n",
    "        if feature_soil.value:\n",
    "            active_feat_names.append(\"Soil Type\")\n",
    "        print(f\"Selected features: {active_feat_names} (count={len(selected_cols)})\")\n",
    "        print(f\"Preprocessing: {preproc_dropdown.value}\")\n",
    "        print(f\"Balanced training: {balance_dropdown.value}\")\n",
    "        print(f\"Model: {model_dropdown.value}\")\n",
    "        if model_dropdown.value == \"MLP\":\n",
    "            print(f\" - Neurons: {mlp_neurons.value}, alpha={mlp_alpha.value:g}\")\n",
    "        elif model_dropdown.value == \"Decision Tree\":\n",
    "            print(f\" - Max depth: {dt_max_depth.value}, ccp_alpha={dt_ccp_alpha.value:g}\")\n",
    "        elif model_dropdown.value == \"Random Forest\":\n",
    "            print(f\" - Trees: {rf_n_estimators.value}, Max depth: {rf_max_depth.value}, min_samples_leaf={rf_min_samples_leaf.value}\")\n",
    "        elif model_dropdown.value == \"Logistic Regression\":\n",
    "            print(f\" - Penalty: {lr_penalty.value}\")\n",
    "\n",
    "        print(\"\\n=== Performance (Train set) ===\")\n",
    "        print(f\"Train Accuracy: {tr_acc:.4f}\")\n",
    "        \n",
    "        print(\"\\n=== Performance (Test set) ===\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision (macro): {prec:.4f}\")\n",
    "        print(f\"Recall (macro): {rec:.4f}\")\n",
    "        print(f\"F1-score (macro): {f1m:.4f}\")\n",
    "        print(f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"\\nTraining time: {t_train:.3f} s\")\n",
    "        print(f\"Inference time (predict): {t_pred:.3f} s\")\n",
    "\n",
    "        #print(\"\\nConfusion matrix (rows: true, cols: predicted):\")\n",
    "        #cm_df = pd.DataFrame(cm)\n",
    "        #display(cm_df)\n",
    "        \n",
    "        print(\"\\nConfusion matrix:\")\n",
    "        labels = np.unique(y_test)\n",
    "        fig, ax = plt.subplots(figsize=(5, 5), dpi=120)\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, cmap=\"Blues\", normalize='true', values_format='.3f', colorbar=True, ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "run_button.on_click(run_experiment)\n",
    "\n",
    "# -----------------------------\n",
    "# Defaults for reset\n",
    "# -----------------------------\n",
    "DEFAULTS = {\n",
    "    \"preproc\": \"Center & Scale\",\n",
    "    \"balancing\": \"None\",\n",
    "\n",
    "    \"feature_cols\": [True] * 10,  # first 10 features checked\n",
    "    \"feature_wild\": True,\n",
    "    \"feature_soil\": True,\n",
    "\n",
    "    \"model\": \"Logistic Regression\",\n",
    "    \"logreg_degree\": 1,\n",
    "    \"lr_penalty\": \"None\",\n",
    "\n",
    "    \"mlp_neurons\": 64,\n",
    "    \"mlp_alpha\": 1e-2,\n",
    "\n",
    "    \"dt_max_depth\": 20,\n",
    "    \"dt_ccp_alpha\": 0.0,\n",
    "\n",
    "    \"rf_n_estimators\": 100,\n",
    "    \"rf_max_depth\": 20,\n",
    "    \"rf_min_samples_leaf\": 1,\n",
    "}\n",
    "\n",
    "reset_button = widgets.Button(\n",
    "    description=\"Reset to defaults\",\n",
    "    button_style=\"warning\",\n",
    "    icon=\"refresh\"\n",
    ")\n",
    "\n",
    "def reset_to_defaults(_):\n",
    "    # Data preparation\n",
    "    preproc_dropdown.value = DEFAULTS[\"preproc\"]\n",
    "    balance_dropdown.value = DEFAULTS[\"balancing\"]\n",
    "\n",
    "    # Feature selection\n",
    "    for cb, val in zip(feature_checkboxes, DEFAULTS[\"feature_cols\"]):\n",
    "        cb.value = val\n",
    "    feature_wild.value = DEFAULTS[\"feature_wild\"]\n",
    "    feature_soil.value = DEFAULTS[\"feature_soil\"]\n",
    "\n",
    "    # Model selection\n",
    "    model_dropdown.value = DEFAULTS[\"model\"]\n",
    "\n",
    "    # Model-specific hyperparameters\n",
    "    logreg_degree.value = DEFAULTS[\"logreg_degree\"]\n",
    "    lr_penalty.value = DEFAULTS[\"lr_penalty\"]\n",
    "\n",
    "    mlp_neurons.value = DEFAULTS[\"mlp_neurons\"]\n",
    "    mlp_alpha.value = DEFAULTS[\"mlp_alpha\"]\n",
    "\n",
    "    dt_max_depth.value = DEFAULTS[\"dt_max_depth\"]\n",
    "    dt_ccp_alpha.value = DEFAULTS[\"dt_ccp_alpha\"]\n",
    "\n",
    "    rf_n_estimators.value = DEFAULTS[\"rf_n_estimators\"]\n",
    "    rf_max_depth.value = DEFAULTS[\"rf_max_depth\"]\n",
    "    rf_min_samples_leaf.value = DEFAULTS[\"rf_min_samples_leaf\"]\n",
    "\n",
    "reset_button.on_click(reset_to_defaults)\n",
    "\n",
    "# -----------------------------\n",
    "# Display UI\n",
    "# -----------------------------\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Forest Covertype ML Playground</h2>\"),\n",
    "    accordion,\n",
    "    widgets.HBox([run_button, reset_button]),\n",
    "    output\n",
    "])\n",
    "\n",
    "display(widgets.VBox([data_ui, ui]))\n",
    "\n",
    "run_dataset(\"Subset A\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
