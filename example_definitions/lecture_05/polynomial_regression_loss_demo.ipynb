{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d97687-a84d-4f81-9923-221a41895c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, HuberRegressor\n",
    "from ipywidgets import interactive, IntSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "# Diagram settings\n",
    "title_font_size = 12\n",
    "axis_font_size = 12\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def plot_description(text):\n",
    "    print(f\"\\nDescription:\\n{text}\\n\")\n",
    "\n",
    "\n",
    "def polynomial_regression_loss_demo(num_outliers=50, poly_estimator_degree=3):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create nonlinear dataset (quadratic + noise)\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "    y = 3 * X.squeeze()**3 - 3 * X.squeeze()**2 - 4 * X.squeeze() + np.random.randn(n_samples) * 2\n",
    "\n",
    "    # Add outliers\n",
    "    outlier_indices = np.random.choice(n_samples, num_outliers, replace=False)\n",
    "    y[outlier_indices] += 30 * np.random.randn(num_outliers)\n",
    "    \n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_estimator_degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Normalize for fairness\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train models with different loss functions\n",
    "    # ---------------------------------------------------\n",
    "    ols = LinearRegression().fit(X_scaled, y)\n",
    "    sgd_mae = SGDRegressor(loss='epsilon_insensitive', epsilon=0.0, \n",
    "                           max_iter=10000, tol=1e-3, random_state=42).fit(X_scaled, y)\n",
    "    huber_reg = HuberRegressor(epsilon=1.0, max_iter=1000).fit(X_scaled, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Predict on dense grid for smooth curves\n",
    "    # ---------------------------------------------------\n",
    "    X_line = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "    X_line_poly = poly.transform(X_line)\n",
    "    X_line_scaled = scaler.transform(X_line_poly)\n",
    "    \n",
    "    y_ols = ols.predict(X_line_scaled)\n",
    "    y_mae = sgd_mae.predict(X_line_scaled)\n",
    "    y_huber = huber_reg.predict(X_line_scaled)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 4. Visualization\n",
    "    # ---------------------------------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color='gray', alpha=0.4, label=\"Data\")\n",
    "    plt.plot(X_line, y_ols, label=\"MSE\", linewidth=2)\n",
    "    plt.plot(X_line, y_mae, label=\"MAE\", linewidth=2)\n",
    "    plt.plot(X_line, y_huber, label=\"Huber\", linewidth=2)\n",
    "    \n",
    "    plt.title(\"Polynomial Regression with Different Loss Functions\", fontsize=16)\n",
    "    plt.xlabel(\"X\", fontsize=14)\n",
    "    plt.ylabel(\"y\", fontsize=14)\n",
    "    plt.ylim([-125,100])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Interactive control panel (same style as before)\n",
    "# ---------------------------------------------------\n",
    "def polynomial_regression_loss_demo_interact():\n",
    "    plot_description(\"Three polynomial models (with MSE/MAE/Huber loss) are fitted to the data. Play around with the\"\n",
    "                     \" number of outliers and the polynomial degree of the models. Notice differences in the models.\"\n",
    "                    \" You will see how the Huber loss model keeps balance between the MSE and MAE loss model.\")\n",
    "    \n",
    "    num_outliers_slider = IntSlider(\n",
    "        value=10, min=0, max=100, step=5,\n",
    "        description=\"Number of Outliers\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    poly_estimator_degree = IntSlider(\n",
    "        value=3, min=1, max=6, step=1,\n",
    "        description=\"Polyn. Estimator Degree\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "\n",
    "    interactive_plot = interactive(\n",
    "        polynomial_regression_loss_demo,\n",
    "        num_outliers=num_outliers_slider,\n",
    "        poly_estimator_degree=poly_estimator_degree\n",
    "    )\n",
    "\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be96d14-dcd4-453c-bd51-2f5a5796d40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
