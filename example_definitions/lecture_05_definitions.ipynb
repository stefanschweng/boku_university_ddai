{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7faa7b-3588-4536-bc56-abafe5aff250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Diagram settings\n",
    "title_font_size = 12\n",
    "axis_font_size = 12\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def plot_description(text):\n",
    "    print(f\"\\nDescription:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7649dc27-3636-4f09-8c7e-a5373f942b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss functions: MSE vs. MAE vs. Huber\n",
    "def loss_functions_visualize():    \n",
    "    # ---------------------------------------------------\n",
    "    # 1. Visualize MSE, MAE, and Huber Loss\n",
    "    # ---------------------------------------------------\n",
    "    errors = np.linspace(-2, 2, 200)\n",
    "    delta = 1.0  # Huber threshold\n",
    "    \n",
    "    mse = errors**2\n",
    "    mae = np.abs(errors)\n",
    "    huber = np.where(np.abs(errors) <= delta,\n",
    "                     0.5 * errors**2,\n",
    "                     delta * (np.abs(errors) - 0.5 * delta))\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(errors, mse, label=\"MSE\", linewidth=2)\n",
    "    plt.plot(errors, mae, label=\"MAE\", linewidth=2)\n",
    "    plt.plot(errors, huber, label=\"Huber (Î´=1)\", linewidth=2)\n",
    "    plt.title(\"Loss Functions vs. Prediction Error\", fontsize=title_font_size)\n",
    "    plt.xlabel(\"Prediction Error (y - Å·)\", fontsize=axis_font_size)\n",
    "    plt.ylabel(\"Loss\", fontsize=axis_font_size)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcc99e5-5b08-4ac9-925a-a2a44e4dc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, FloatSlider, IntSlider, VBox, Label, Layout\n",
    "\n",
    "def linear_reg_loss_demo(num_outliers=50):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create a simple regression dataset\n",
    "    # ---------------------------------------------------\n",
    "    n_samples = 500\n",
    "    X, y = make_regression(n_samples=n_samples, n_features=1, noise=15, random_state=42)\n",
    "\n",
    "    # Add outliers controlled by sliders\n",
    "    outlier_indices = np.random.choice(n_samples, num_outliers, replace=False)\n",
    "    y[outlier_indices] += 200 * np.random.randn(num_outliers)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train models with different loss functions\n",
    "    # ---------------------------------------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    ols = LinearRegression().fit(X_scaled, y)\n",
    "\n",
    "    sgd_mae = SGDRegressor(loss='epsilon_insensitive', epsilon=0.0, \n",
    "                           max_iter=10000, tol=1e-3, random_state=42).fit(X_scaled, y)\n",
    "    \n",
    "    huber_reg = HuberRegressor(epsilon=1.0, max_iter=1000).fit(X_scaled, y)\n",
    "    \n",
    "    # Predictions\n",
    "    x_line = np.linspace(X_scaled.min(), X_scaled.max(), 100).reshape(-1, 1)\n",
    "    y_ols = ols.predict(x_line)\n",
    "    y_mae = sgd_mae.predict(x_line)\n",
    "    y_huber = huber_reg.predict(x_line)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Plot results\n",
    "    # ---------------------------------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_scaled, y, color='gray', alpha=0.4, label=\"Data\")\n",
    "    plt.plot(x_line, y_ols, label=\"MSE\", linewidth=2)\n",
    "    plt.plot(x_line, y_mae, label=\"MAE\", linewidth=2)\n",
    "    plt.plot(x_line, y_huber, label=\"Huber\", linewidth=2)\n",
    "    plt.title(\"Effect of Different Loss Functions on Fit\", fontsize=title_font_size)\n",
    "    plt.xlabel(\"X\", fontsize=axis_font_size)\n",
    "    plt.ylabel(\"y\", fontsize=axis_font_size)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "    # 4. Create clearly labeled, wide sliders inside a titled block\n",
    "    # ---------------------------------------------------\n",
    "def linear_reg_loss_functions_demo_interact():\n",
    "    plot_description(\"Three linear models using different loss functions (MSE/MAE/Huber) are fitted to the samples (gray dots). \"\n",
    "                     \"Increase the number of outliers in the dataset and notice differences in the models. \"\n",
    "                     \"Takeaway: Model predictions vary based on the loss function that was used to train the model.\")\n",
    "    \n",
    "    num_outliers_slider = IntSlider(\n",
    "        value=50, min=0, max=500, step=5,\n",
    "        description=\"Number of Outliers\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "\n",
    "    interactive_plot = interactive(\n",
    "        linear_reg_loss_demo,\n",
    "        num_outliers=num_outliers_slider\n",
    "    )\n",
    "\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e93104-7774-46bb-97e2-ffe6f259978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, HuberRegressor\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "def polynomial_regression_loss_demo(num_outliers=50, poly_estimator_degree=3):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create nonlinear dataset (quadratic + noise)\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    n_samples = 400\n",
    "    X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "    y = 3 * X.squeeze()**3 - 3 * X.squeeze()**2 - 4 * X.squeeze() + np.random.randn(n_samples) * 2\n",
    "\n",
    "    # Add outliers\n",
    "    outlier_indices = np.random.choice(n_samples, num_outliers, replace=False)\n",
    "    y[outlier_indices] += 30 * np.random.randn(num_outliers)\n",
    "    \n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_estimator_degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Normalize for fairness\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train models with different loss functions\n",
    "    # ---------------------------------------------------\n",
    "    ols = LinearRegression().fit(X_scaled, y)\n",
    "    sgd_mae = SGDRegressor(loss='epsilon_insensitive', epsilon=0.0, \n",
    "                           max_iter=10000, tol=1e-3, random_state=42).fit(X_scaled, y)\n",
    "    huber_reg = HuberRegressor(epsilon=1.0, max_iter=1000).fit(X_scaled, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Predict on dense grid for smooth curves\n",
    "    # ---------------------------------------------------\n",
    "    X_line = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "    X_line_poly = poly.transform(X_line)\n",
    "    X_line_scaled = scaler.transform(X_line_poly)\n",
    "    \n",
    "    y_ols = ols.predict(X_line_scaled)\n",
    "    y_mae = sgd_mae.predict(X_line_scaled)\n",
    "    y_huber = huber_reg.predict(X_line_scaled)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 4. Visualization\n",
    "    # ---------------------------------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color='gray', alpha=0.4, label=\"Data\")\n",
    "    plt.plot(X_line, y_ols, label=\"MSE\", linewidth=2)\n",
    "    plt.plot(X_line, y_mae, label=\"MAE\", linewidth=2)\n",
    "    plt.plot(X_line, y_huber, label=\"Huber\", linewidth=2)\n",
    "    \n",
    "    plt.title(\"Polynomial Regression with Different Loss Functions\", fontsize=16)\n",
    "    plt.xlabel(\"X\", fontsize=14)\n",
    "    plt.ylabel(\"y\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Interactive control panel (same style as before)\n",
    "# ---------------------------------------------------\n",
    "def polynomial_regression_loss_demo_interact():\n",
    "    plot_description(\"Three polynomial models (with MSE/MAE/Huber loss) are fitted to the data. Play around with the\"\n",
    "                     \" number of outliers and the polynomial degree of the models. Notice differences in the models.\")\n",
    "    \n",
    "    num_outliers_slider = IntSlider(\n",
    "        value=10, min=0, max=200, step=5,\n",
    "        description=\"Number of Outliers\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    poly_estimator_degree = IntSlider(\n",
    "        value=3, min=1, max=6, step=1,\n",
    "        description=\"Polyn. Estimator Degree\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "\n",
    "    interactive_plot = interactive(\n",
    "        polynomial_regression_loss_demo,\n",
    "        num_outliers=num_outliers_slider,\n",
    "        poly_estimator_degree=poly_estimator_degree\n",
    "    )\n",
    "\n",
    "    display(ui_box, interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99294f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "def polynomial_overfitting_demo(poly_degree=1, noise_level=10.0, num_samples=60):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create synthetic data from a known nonlinear function\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(-3, 3, num_samples).reshape(-1, 1)\n",
    "    y_true = np.sin(X).ravel()\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    y = y_true + np.random.randn(num_samples) * (noise_level / 10)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Build and fit polynomial regression model\n",
    "    # ---------------------------------------------------\n",
    "    poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    model = LinearRegression().fit(X_scaled, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Generate smooth prediction curve\n",
    "    # ---------------------------------------------------\n",
    "    X_line = np.linspace(-3, 3, 300).reshape(-1, 1)\n",
    "    X_line_poly = poly.transform(X_line)\n",
    "    X_line_scaled = scaler.transform(X_line_poly)\n",
    "    y_pred = model.predict(X_line_scaled)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 4. Plot results\n",
    "    # ---------------------------------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color=\"gray\", alpha=0.7, label=\"Training Data\")\n",
    "    plt.plot(X_line, np.sin(X_line), color=\"green\", linewidth=2, label=\"True Function (sin x)\")\n",
    "    plt.plot(X_line, y_pred, color=\"red\", linewidth=2, label=f\"Polynomial Fit (degree={poly_degree})\")\n",
    "    \n",
    "    plt.title(\"Effect of Model Complexity (Overfitting Demonstration)\", fontsize=16)\n",
    "    plt.xlabel(\"X\", fontsize=14)\n",
    "    plt.ylabel(\"y\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Interactive controls\n",
    "# ---------------------------------------------------\n",
    "def polynomial_overfitting_demo_interact():\n",
    "    plot_description(\"Demonstration of overfitting with a polynomial model. The true function to be fitted is a sinus curve (green line).\"\n",
    "                     \" The samples (gray dots) are the (noisy) measurements taken from the true function. The task here is to \"\n",
    "                     \"create a polynomial model (red line) that resembles the true function as closely as possible, based on the measured\"\n",
    "                     \" samples. Increase the polynomial degree of the model and notice how the model is overfitting the samples \"\n",
    "                    \"starting from degree > 4.\")\n",
    "    \n",
    "    degree_slider = IntSlider(\n",
    "        value=1, min=1, max=20, step=1,\n",
    "        description=\"Polynomial Degree\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "    noise_slider = FloatSlider(\n",
    "        value=10.0, min=0.0, max=50.0, step=2.0,\n",
    "        description=\"Noise Level\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "    sample_slider = IntSlider(\n",
    "        value=60, min=20, max=200, step=10,\n",
    "        description=\"Number of Samples\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "\n",
    "    interactive_plot = interactive(\n",
    "        polynomial_overfitting_demo,\n",
    "        poly_degree=degree_slider,\n",
    "        noise_level=noise_slider,\n",
    "        num_samples=sample_slider\n",
    "    )\n",
    "\n",
    "    display(ui_box, interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b0fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipywidgets import interactive, FloatSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "def binary_classification_demo(separation=1.0):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create binary dataset\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    X, y = make_classification(\n",
    "        n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
    "        n_clusters_per_class=1, class_sep=separation, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train logistic regression model\n",
    "    # ---------------------------------------------------\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Visualize decision boundary\n",
    "    # ---------------------------------------------------\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, 1-Z, cmap=\"RdYlBu\", alpha=0.5)\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], color=\"blue\", label=\"Class 0\", edgecolor=\"black\", linewidth=0.5)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], color=\"red\", label=\"Class 1\", edgecolor=\"black\", linewidth=0.5)\n",
    "    plt.title(\"Binary Classification with Logistic Regression\", fontsize=16)\n",
    "    plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "    plt.ylabel(\"Feature 2\", fontsize=14)\n",
    "    plt.legend(title=\"Ground Truth\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def binary_classification_demo_interact():\n",
    "    # ---------------------------------------------------\n",
    "    # 4. Visualize Binary Cross-Entropy loss function\n",
    "    # ---------------------------------------------------\n",
    "    p = np.linspace(0.001, 0.999, 200)\n",
    "    loss_positive = -np.log(p)\n",
    "    loss_negative = -np.log(1 - p)\n",
    "\n",
    "    plot_description(\"We are looking at a binary classification problem (i.e., assigning a sample to one of two classes). The Figure\"\n",
    "                     \" below shows an example visualization of the Binary Cross-Entropy (BCE) loss function that was used to train the \"\n",
    "                     \"logistic regression model. The BCE function calculates the loss values based on the orange line (for samples of class 0) \"\n",
    "                    \"or based on the blue line (for samples of class 1).\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(p, loss_positive, label=\"Loss (True Label = 1)\", linewidth=2)\n",
    "    plt.plot(p, loss_negative, label=\"Loss (True Label = 0)\", linewidth=2)\n",
    "    plt.title(\"Binary Cross-Entropy Loss\", fontsize=14)\n",
    "    plt.xlabel(\"Predicted Probability (p)\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plot_description(\"Looking at the plot below, the blue and rot dots represent training samples of class 0 and class 1 respectively.\"\n",
    "                     \" The background shows prediction probabilities of the logisitic regression model.\"\n",
    "                     \" Bluish background: High probability for Class 0. Reddish background: High probability for Class 1. \\n\"\n",
    "                     \"The model assigns class 0 or 1 to a new \\\"unseen\\\" sample, based on these probabilities. \"\n",
    "                     \"Notice how prediction uncertainty increases as training samples of both classes get mixed (play around with the slider below).\")\n",
    "    \n",
    "    # --- Interactive control ---\n",
    "    sep_slider = FloatSlider(\n",
    "        value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "        description=\"Class Separation\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px'),\n",
    "    )\n",
    "\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "    interactive_plot = interactive(binary_classification_demo, separation=sep_slider)\n",
    "\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6357a5f-eac1-4345-b981-3fd8e6cc2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipywidgets import interactive, FloatSlider, VBox, Label, Layout\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def multi_class_classification_demo(separation=1.0):\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Create multi-class dataset\n",
    "    # ---------------------------------------------------\n",
    "    np.random.seed(42)\n",
    "    X, y = make_classification(\n",
    "        n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n",
    "        n_classes=3, n_clusters_per_class=1, class_sep=separation, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2. Train logistic regression model for multi-class\n",
    "    # ---------------------------------------------------\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 3. Visualize decision boundaries\n",
    "    # ---------------------------------------------------\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    Z_probabilities = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        \n",
    "    Z_max_proba = Z_probabilities.max(axis=1)\n",
    "    Z_max_proba = Z_max_proba.reshape(xx.shape)\n",
    "    \n",
    "    Z_predictions = np.argmax(Z_probabilities, axis=1)\n",
    "    Z_predictions = Z_predictions.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plt.suptitle(\"Multi-Class Classification with Logistic Regression\", fontsize=16)\n",
    "    titles = [\"Max. Prediction Probabilities\", \"Model Decision Boundaries\"]\n",
    "    \n",
    "    for i, title in enumerate(titles):\n",
    "        if i == 0:\n",
    "            zz = Z_max_proba\n",
    "            cmap = \"Blues\"\n",
    "        else:\n",
    "            zz = Z_predictions\n",
    "            cmap = \"viridis\"\n",
    "\n",
    "        ax[i].set_title(title, fontsize=12)\n",
    "        ax[i].contourf(xx, yy, zz, cmap=cmap , alpha=0.5)\n",
    "        ax[i].scatter(X[y==0, 0], X[y==0, 1], color=\"blue\", label=\"Class 0\", edgecolor=\"black\", linewidth=0.5)\n",
    "        ax[i].scatter(X[y==1, 0], X[y==1, 1], color=\"red\", label=\"Class 1\", edgecolor=\"black\", linewidth=0.5)\n",
    "        ax[i].scatter(X[y==2, 0], X[y==2, 1], color=\"green\", label=\"Class 2\", edgecolor=\"black\", linewidth=0.5)\n",
    "        ax[i].set_xlabel(\"Feature 1\", fontsize=12)\n",
    "        ax[i].set_ylabel(\"Feature 2\", fontsize=12)\n",
    "        ax[i].legend(title=\"Ground Truth\")\n",
    "        ax[i].grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def multi_class_classification_demo_interact():\n",
    "    plot_description(\"Example for Multi-Class Classification with a logistic regression model trained using the cross-entropy loss function.\"\n",
    "                     \" Training samples are shown in blue (class 0), red (class 1) and green (class 2). The model outputs three probability values,\"\n",
    "                     \" one for each class, based on the sample's input features (Feature 1 & 2).\\n\\n\"\n",
    "                     \"Left plot: The colored background shows the maximum probability value for each combination of Feature 1 & 2. The darker the \"\n",
    "                     \"background, the higher the probability. Notice how the light area (i.e., the model's uncertainty) increases as training samples\"\n",
    "                     \" of different classes mix (play around with the slider below).\\n\\n\"\n",
    "                     \"Right plot: The background colors of this plot show the model decisions (i.e., assignment of input feature combinations to one class).\"\n",
    "                     \" Purple area => Class 0, Turquoise area => Class 1, Yellow area => Class 2.\")\n",
    "    \n",
    "    # --- Interactive control ---\n",
    "    sep_slider = FloatSlider(\n",
    "        value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "        description=\"Class Separation\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "    interactive_plot = interactive(multi_class_classification_demo, separation=sep_slider)\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd564f62-c081-4221-ae25-1d43d65c2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Using cached scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn-extra) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn-extra) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn-extra) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/micromamba/lib/python3.11/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.6.0)\n",
      "Using cached scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Installing collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from ipywidgets import interact, FloatSlider, VBox, HTML\n",
    "\n",
    "!pip install scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "\n",
    "def clustering_demo(class_separation=0.0):\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Generate two rectangular clusters\n",
    "    cluster1 = np.random.uniform(low=[0, 0], high=[4, 4], size=(200, 2))\n",
    "    cluster2 = np.random.uniform(low=[4 - class_separation, 0],\n",
    "                                 high=[6 - class_separation, 4], size=(200, 2))\n",
    "    X = np.vstack([cluster1, cluster2])\n",
    "    \n",
    "    # Fit models\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0).fit(X)\n",
    "    kmedians = KMedoids(n_clusters=2, metric='manhattan', random_state=0).fit(X)\n",
    "    \n",
    "    # Sort clusters by x-coordinate of their centers (to ensure consistent colors)\n",
    "    def sort_clusters(model):\n",
    "        centers = model.cluster_centers_\n",
    "        order = np.argsort(centers[:, 0])\n",
    "        labels_sorted = np.zeros_like(model.labels_)\n",
    "        for new_label, old_label in enumerate(order):\n",
    "            labels_sorted[model.labels_ == old_label] = new_label\n",
    "        return labels_sorted, centers[order]\n",
    "    \n",
    "    labels_kmeans, centers_kmeans = sort_clusters(kmeans)\n",
    "    labels_kmedians, centers_kmedians = sort_clusters(kmedians)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    titles = [\"K-Means (L2 loss)\", \"K-Medians (L1 loss)\"]\n",
    "    \n",
    "    for i, (labels, centers, title) in enumerate(\n",
    "        zip([labels_kmeans, labels_kmedians],\n",
    "            [centers_kmeans, centers_kmedians],\n",
    "            titles)\n",
    "    ):\n",
    "        ax[i].scatter(X[labels==0, 0], X[labels==0, 1], color=\"blue\" if \"K-Means\" in title else \"green\", label=\"Cluster 0\", s=40, alpha=0.7)\n",
    "        ax[i].scatter(X[labels==1, 0], X[labels==1, 1], color=\"orange\" if \"K-Means\" in title else \"purple\", label=\"Cluster 1\", s=40, alpha=0.7)\n",
    "        ax[i].scatter(centers[:, 0], centers[:, 1],\n",
    "                      c=\"red\", s=200 if \"K-Means\" in title else 100,\n",
    "                      marker=\"X\" if \"K-Means\" in title else \"D\",\n",
    "                      edgecolor=\"black\", label=\"Means\" if \"K-Means\" in title else \"Medians\")\n",
    "        ax[i].set_title(title, fontsize=14)\n",
    "        ax[i].set_xlim(-1, 7)\n",
    "        ax[i].set_ylim(-1, 6)\n",
    "        ax[i].grid(True)\n",
    "        ax[i].legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def clustering_demo_interact():\n",
    "    plot_description(\"Clustering Demo: K-Means (trained using L2 loss) vs. K-Medians (trained using L1 loss). Note that \"\n",
    "                     \"these plots demonstrate an example of unsupervised learning (in contrast to previous plots). \"\n",
    "                     \"The task of the K-Means and K-Medians algorithms is to find clusters independent of any pre-defined ground truth labels.\\n\\n\"\n",
    "                    \"Left Plot: Clusters found by K-Means with respective cluster means (red crosses).\\n\\n\"\n",
    "                    \"Right Plot: Clusters found by K-Medians with respective cluster medians (red squares).\\n\\n\"\n",
    "                    \"In this case, we instructed both algorithms to find two distinct clusters. Notice how the identified\\n\"\n",
    "                    \" clusters differ as the sample distribution changes (play around with the slider below).\")\n",
    "    \n",
    "    # --- Interactive control ---\n",
    "    sep_slider2 = FloatSlider(\n",
    "        value=0.0, min=-1.0, max=4.0, step=0.1,\n",
    "        description=\"Move Samples\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=Layout(width='500px')\n",
    "    )\n",
    "    ui_box = VBox([\n",
    "        Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 0 0\")),\n",
    "    ])\n",
    "    interactive_plot = interactive(clustering_demo, class_separation=sep_slider2)\n",
    "    display(ui_box, interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a05c142-8c28-4919-b79c-58a56bada859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Dropdown, FloatSlider, IntSlider\n",
    "from ipywidgets import interactive, VBox, Output, Label\n",
    "\n",
    "# --- Define the parabola function ---\n",
    "def f(x):\n",
    "    return x**2  # simple convex function\n",
    "\n",
    "def df(x):\n",
    "    return 2*x  # derivative\n",
    "\n",
    "# --- Optimization routine ---\n",
    "def optimize(optimizer=\"GD\", lr=0.1, epochs=20, momentum=0.9):\n",
    "    x = -8 # np.random.uniform(-8, 8)  # random start\n",
    "    v = 0  # momentum buffer\n",
    "    history = [x]\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grad = df(x)\n",
    "\n",
    "        if optimizer == \"GD\":\n",
    "            x = x - lr * grad\n",
    "\n",
    "        elif optimizer == \"Momentum\":\n",
    "            v = momentum * v - lr * grad\n",
    "            x = x + v\n",
    "\n",
    "        history.append(x)\n",
    "\n",
    "    return np.array(history)\n",
    "\n",
    "# --- Plotting function ---\n",
    "def train_and_plot(optimizer, lr, epochs, momentum):\n",
    "    xs = np.linspace(-11, 11, 500)\n",
    "    ys = f(xs)\n",
    "\n",
    "    path = optimize(optimizer, lr, epochs, momentum)\n",
    "    path_y = f(path)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(xs, ys, label=\"f(x) = xÂ²\", color=\"blue\", alpha=0.5)\n",
    "    plt.plot(path, path_y, color=\"red\", linestyle=\"--\", alpha=0.7, zorder=3)\n",
    "    plt.scatter(path[:-1], path_y[:-1], s=60, label=\"Steps\", marker=\"o\", facecolors='none', edgecolors='black', zorder=4)\n",
    "    plt.scatter(path[-1], path_y[-1], color=\"red\", s=80, label=\"Final Step\", marker=\"x\", zorder=5)\n",
    "\n",
    "    plt.title(f\"Params: lr={lr:.2f}, epochs={epochs}, momentum={momentum:.2f}\" if optimizer==\"Momentum\" else f\"{optimizer}: lr={lr:.2f}, epochs={epochs}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.legend()\n",
    "    plt.xlim([-12.5, 12.5])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- Widgets ---\n",
    "# Define widgets with default values\n",
    "optimizer_widget = Dropdown(options=[\"GD\", \"Momentum\"], value=\"GD\", description=\"Optimizer\")\n",
    "\n",
    "# Store defaults for reset\n",
    "defaults = {\n",
    "    \"lr\": 0.05,\n",
    "    \"epochs\": 10,\n",
    "    \"momentum\": 0.75\n",
    "}\n",
    "\n",
    "lr_widget = FloatSlider(value=defaults[\"lr\"], min=0.01, max=1.1, step=0.01, description=\"Learning rate\", readout_format=\".2f\")\n",
    "epochs_widget = IntSlider(value=defaults[\"epochs\"], min=5, max=100, step=5, description=\"Epochs\")\n",
    "momentum_widget = FloatSlider(value=defaults[\"momentum\"], min=0.1, max=0.99, step=0.05, description=\"Momentum\", readout_format=\".2f\")\n",
    "momentum_widget.layout.visibility = 'hidden'\n",
    "\n",
    "def on_optimizer_change(change):\n",
    "    if change[\"name\"] == \"value\":\n",
    "        lr_widget.value = defaults[\"lr\"]\n",
    "        epochs_widget.value = defaults[\"epochs\"]\n",
    "        momentum_widget.value = defaults[\"momentum\"]\n",
    "\n",
    "        if change[\"new\"] == \"Momentum\":\n",
    "            momentum_widget.layout.visibility = 'visible'\n",
    "        else:\n",
    "            momentum_widget.layout.visibility = 'hidden'\n",
    "\n",
    "optimizer_widget.observe(on_optimizer_change, names=\"value\")\n",
    "\n",
    "label = Label(value=\"ðŸ“Š Controls\", layout=Layout(margin=\"0 0 8px 0\"))\n",
    "\n",
    "# Add left margin (indent) to the widgets you want to indent\n",
    "margin_left = 40\n",
    "optimizer_widget.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "lr_widget.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "epochs_widget.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "momentum_widget.layout.margin = f\"0 0 0 {margin_left}px\"\n",
    "\n",
    "ui = VBox([label, optimizer_widget, lr_widget, epochs_widget, momentum_widget])\n",
    "out = Output()\n",
    "\n",
    "def wrapped_train_and_plot(optimizer, lr, epochs, momentum):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        train_and_plot(optimizer, lr, epochs, momentum)\n",
    "\n",
    "def gradient_descent_interact():\n",
    "    plot_description(\"Gradient descent (GD) demonstration. Use the dropdown to switch between standard GD and GD+Momentum.\"\n",
    "                    \" Adjust learning rate, number of epochs and momentum. Notice how the process diverges when learning rate > 1.\")\n",
    "    \n",
    "    interactive_plot = interactive(\n",
    "        wrapped_train_and_plot,\n",
    "        optimizer=optimizer_widget,\n",
    "        lr=lr_widget,\n",
    "        epochs=epochs_widget,\n",
    "        momentum=momentum_widget,\n",
    "    )\n",
    "    \n",
    "    display(ui, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
